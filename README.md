# Image-Caption
Relating words to an image is a major goal in image captioning. While recent technological advances has enabled us to generate description learning from the huge corpus of words, however, more often than not we fall short of words when it comes to convey message through an image. Through this paper we are targeting to find a reasonable ground in generating customized and personalized captions after learning features of an image and generating captions. While the success of these methods is encouraging, they all share one key limitation: detail. By only describing images with a single high-level sentence, there is a fundamental upper-bound on the quantity and quality of information approaches can produce. In particular, we are interested in the generating longer, richer sentences and paragraphs that could convey a story/message rather than description of image. In this paper we explore and summaries few of the existing state-of-art techniques for image captioning and image paragraph captioning using the novel data from the Instagram. We explore standard MLE based Encoder-Decoder architecture for captioning, Hierarchical RNNs for paragraph captioning, and another less explored approach based on Conditional Generative Adversarial Networks.
